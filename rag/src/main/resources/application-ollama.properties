# Ollama Configuration

# When Backend is running in Docker (NEED TO CHANGE)
# spring.ai.ollama.base-url=http://host.docker.internal:11434

# When Backend is running locally (NEED TO CHANGE)
spring.ai.ollama.base-url=http://localhost:11434

spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=5m
spring.ai.ollama.embedding.enabled=true

# Model configurations
spring.ai.ollama.chat.options.model=phi3:mini
# spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.embedding.options.model=nomic-embed-text

# Uncomment this if your Backend is running locally and you want to use Ollama in a Docker container along with PostgreSQL (NEED TO CHANGE)
#spring.docker.compose.file=compose-ollama.yaml